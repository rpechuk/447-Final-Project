{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary code package for paper submission: 'Semantically Informed Slang Interpretation'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the supplementary code package for 'Semantically Informed Slang Interpretation'. Since we cannot publically release all revelant datasets used in the study due to copyright terms, the purpose of this notebook is to provide an illustration of how the main results from the paper can be reproduced. Specifically, the code package includes all required non-standard code dependencies and code in this notebook show how results can be reproduced using these libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We include custom versions of code released from previous work in the following directories:\n",
    "\n",
    "- **/CatGO** - A categorization library from Zhewei Sun, Richard Zemel, and Yang Xu, 'Slang generation as categorization', 2019.\n",
    "- **/ilm** - A pre-trained GPT-2 based language infill model from Chris Donahue, Mina Lee, and Percy Liang, 'Enabling language models to fill in the blanks', 2020\n",
    "- **/slanggen** - A library for training contrastively learned slang sense embeddings from Zhewei Sun, Richard Zemel, and Yang Xu, 'A computational framework for slang generation', 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a list of scientific Python packages you'll need:\n",
    "\n",
    "- numpy\n",
    "- scipy\n",
    "- nltk\n",
    "- gensim\n",
    "- Flair\n",
    "- PyTorch\n",
    "- transformers\n",
    "- sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "import numpy as np\n",
    "import scipy.spatial.distance as dist\n",
    "import scipy.stats\n",
    "from scipy.stats import norm, mode\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from collections import defaultdict, namedtuple, Counter, defaultdict\n",
    "\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from CatGO.categorize import Categorizer\n",
    "\n",
    "from slanggen.util import *\n",
    "from slanggen.dataloader import UD_Wil_Dataset, OED_Dataset\n",
    "from slanggen.encoder import FTEncoder\n",
    "from slanggen.contrastive import SlangGenTrainer\n",
    "from slanggen.model import SlangGenModel\n",
    "\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "import ilm.tokenize_util\n",
    "import ilm.infer\n",
    "\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the dataset and assosiated indices for data splits. Note that the Oxford Dictionary (OD) data cannot be included so this is just an illustration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oed_data = OED_Dataset('OED_Urban_def_full.npy')\n",
    "\n",
    "ud_dir = '../UDWil/'\n",
    "dataset = UD_Wil_Dataset(ud_dir+'Data/raw/', oed_data, load_oov=True)\n",
    "slang_inds = DataIndex(np.load(ud_dir+'train_ind.npy'), np.load(ud_dir+'dev_ind.npy'), np.load(ud_dir+'test_ind.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra data bookkeeping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dev_ind = np.concatenate((slang_inds.train, slang_inds.dev))\n",
    "\n",
    "ex_sents_test = []\n",
    "ex_sents_inds = []\n",
    "gt_words_test = []\n",
    "\n",
    "for i in range(slang_inds.test.shape[0]):\n",
    "    ind = slang_inds.test[i]\n",
    "    for s in dataset.slang_data[ind].meta_data['context']:\n",
    "        ex_sents_test.append(s.strip())\n",
    "        ex_sents_inds.append(i)\n",
    "        gt_words_test.append(dataset.slang_data[ind].word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain interpretation candidates from a pre-trained language infill model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a POS tagger and tag the blanked out slang expression in the context sentence for every test entry. Note that the slang expression itself is not provided to the tagger to mitigate potential biases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penn_pos_map = {'JJ':'adj',\\\n",
    "                'JJR':'adj',\\\n",
    "                'JJS':'adj',\\\n",
    "                'UH':'interj',\\\n",
    "                'RB':'adv',\\\n",
    "                'RBR':'adv',\\\n",
    "                'RBS':'adv',\\\n",
    "                'WRB':'adv',\\\n",
    "                'NN':'noun',\\\n",
    "                'NNS':'noun',\\\n",
    "                'NNP':'noun',\\\n",
    "                'NNPS':'noun',\\\n",
    "                'MD':'verb',\\\n",
    "                'VB':'verb',\\\n",
    "                'VBD':'verb',\\\n",
    "                'VBG':'verb',\\\n",
    "                'VBN':'verb',\\\n",
    "                'VBP':'verb',\\\n",
    "                'VBZ':'verb'}\n",
    "\n",
    "def conv_penn_pos(tag):\n",
    "    if tag in penn_pos_map:\n",
    "        return penn_pos_map[tag]\n",
    "    return 'other'\n",
    "\n",
    "tagger = SequenceTagger.load('pos')\n",
    "\n",
    "punctuations = '!\"#$%&()\\*\\+,-\\./:;<=>?@[\\\\]^_`{|}~'\n",
    "re_punc = re.compile(r\"[\"+punctuations+r\"]+\")\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return re.compile(r\"(?:^|(?<=\\s))\\S+(?=\\s|$)\").findall(sentence)\n",
    "\n",
    "ex_sents_pos = []\n",
    "\n",
    "for s in ex_sents_test:\n",
    "    \n",
    "    sent = re_punc.sub('', s)\n",
    "    gap_pos = 0\n",
    "    for j, token in enumerate(tokenize(sent)):\n",
    "        if len(token) >= 9:\n",
    "            if token[:9] == 'SLANGAAAP':\n",
    "                gap_pos = j\n",
    "                break\n",
    "    sent = Sentence(re.compile('SLANGAAAP').sub('slanggg', sent))\n",
    "    tagger.predict(sent)\n",
    "    tag_pred = conv_penn_pos(sent.get_spans('pos')[gap_pos].tag)\n",
    "    \n",
    "    ex_sents_pos.append(tag_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a pre-trained language infill model from Donahue et al. (2020). The model can be downloaded from their original repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = 'pretrain/models/sto_ilm' # Change this to where you have placed the pre-trained model\n",
    "MASK_CLS = 'ilm.mask.hierarchical.MaskHierarchical'\n",
    "\n",
    "tokenizer = ilm.tokenize_util.Tokenizer.GPT2\n",
    "with open(os.path.join(MODEL_DIR, 'additional_ids_to_tokens.pkl'), 'rb') as f:\n",
    "    additional_ids_to_tokens = pickle.load(f)\n",
    "additional_tokens_to_ids = {v:k for k, v in additional_ids_to_tokens.items()}\n",
    "try:\n",
    "    ilm.tokenize_util.update_tokenizer(additional_ids_to_tokens, tokenizer)\n",
    "except ValueError:\n",
    "    print('Already updated')\n",
    "print(additional_tokens_to_ids)\n",
    "\n",
    "_blank_id = ilm.tokenize_util.encode(' _', tokenizer)[0]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_DIR)\n",
    "model.eval()\n",
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the language infill model to obtain a list of infilled words for each test example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infill_prob(sentence, n_words=5):\n",
    "\n",
    "    # Preprocess Sentence\n",
    "\n",
    "    context = sentence.replace('[*SLANGAAAP*]', ' _ ')\n",
    "\n",
    "    context_ids = ilm.tokenize_util.encode(context, tokenizer)\n",
    "\n",
    "    context_ids[context_ids.index(_blank_id)] = additional_tokens_to_ids['<|infill_word|>']\n",
    "\n",
    "    # Obtain Probability Distribution from Softmax\n",
    "\n",
    "    probs = ilm.infer.infill_with_ilm(\n",
    "        model,\n",
    "        additional_tokens_to_ids,\n",
    "        context_ids,\n",
    "        num_infills=1).cpu().numpy()[0]\n",
    "\n",
    "    # Collect Words and Probabilities\n",
    "    \n",
    "    top_probs = np.argsort(probs)[::-1]\n",
    "    top_words = ilm.tokenize_util.ids_to_tokens(top_probs[:n_words], tokenizer)\n",
    "  \n",
    "    return probs[top_probs[:n_words]], top_words\n",
    "\n",
    "infill_results_raw = np.asarray([infill_prob(sent, 150) for sent in ex_sents_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First filter out words that contain non-alphanumeric characters. Then, check the part-of-speech (POS) tag predicted from the usage context to see if it matches the candadiate words. Words with matching POS tags are moved to the front of the list. Finally, keep the top 50 candidate words for each test example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alphanum_check(w):\n",
    "    if len(w) == 0:\n",
    "        return False\n",
    "    for c in w:\n",
    "        c_num = ord(c)\n",
    "        if not ((c_num >= 48 and c_num <= 57) or (c_num >= 65 and c_num <= 90) or (c_num >= 97 and c_num <= 122)):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "hist_pos_map = {'ADJ':'adj',\\\n",
    "                'X':'interj',\\\n",
    "                'ADV':'adv',\\\n",
    "                'VERB':'verb',\\\n",
    "                'NOUN':'noun'}\n",
    "\n",
    "def conv_hist_pos(tag):\n",
    "    if tag in hist_pos_map:\n",
    "        return hist_pos_map[tag]\n",
    "    return 'other'\n",
    "\n",
    "# You'll need to download this from the HistWord Project by Hamilton et al. (2016).\n",
    "hist_counts = pickle.load(open('histwords/eng-all/pos/1990-pos_counts.pkl', 'rb'))\n",
    "hist_vocab = set(hist_counts.keys())\n",
    "\n",
    "def get_hist_posdist(word):\n",
    "    results = defaultdict(float)\n",
    "    for key,val in hist_counts[word].items():\n",
    "        results[conv_hist_pos(key)] += val\n",
    "    total = np.sum(list(results.values()))\n",
    "    for key in results.keys():\n",
    "        results[key] /= total\n",
    "    return results\n",
    "\n",
    "def pos_check(word, tag, threshold=0.05):\n",
    "    if word not in hist_vocab:\n",
    "        return False\n",
    "    hist_posdist = get_hist_posdist(word)\n",
    "    return hist_posdist[tag] >= threshold\n",
    "\n",
    "def filter_words(result, tag, n_words=50):\n",
    "    probs = np.asarray(result[0])\n",
    "    words = np.asarray([s.strip() for s in result[1]])\n",
    "    result_mask = np.arange(len(result[1]), dtype=np.int32)\n",
    "    good_pos = set()\n",
    "\n",
    "    for i in range(result_mask.shape[0]):\n",
    "        if not alphanum_check(words[i]):\n",
    "            result_mask[i] = -1\n",
    "        if pos_check(words[i], tag):\n",
    "            good_pos.add(i)\n",
    "\n",
    "    result_mask = np.asarray([i for i in result_mask if i != -1], dtype=np.int32)\n",
    "    \n",
    "    mask_A = np.asarray([i for i in result_mask if i in good_pos], dtype=np.int32)\n",
    "    mask_B = np.asarray([i for i in result_mask if i not in good_pos], dtype=np.int32)\n",
    "    \n",
    "    probs_A = probs[mask_A]\n",
    "    words_A = words[mask_A]\n",
    "    \n",
    "    probs_B = probs[mask_B]\n",
    "    words_B = words[mask_B]\n",
    "\n",
    "    result_dict_A = defaultdict(float)\n",
    "    for i in range(words_A.shape[0]):\n",
    "        result_dict_A[words_A[i].lower()] += float(probs_A[i])\n",
    "    result_dict_B = defaultdict(float)\n",
    "    for i in range(words_B.shape[0]):\n",
    "        result_dict_B[words_B[i].lower()] += float(probs_B[i])\n",
    "\n",
    "    result_keys_A = np.asarray(list(result_dict_A.keys()))\n",
    "    result_values_A = np.asarray(list(result_dict_A.values()))\n",
    "    result_keys_B = np.asarray(list(result_dict_B.keys()))\n",
    "    result_values_B = np.asarray(list(result_dict_B.values()))\n",
    "\n",
    "    sort_ind_A = np.argsort(result_values_A)[::-1]\n",
    "    result_words_A = result_keys_A[sort_ind_A]\n",
    "    result_probs_A = result_values_A[sort_ind_A]\n",
    "    \n",
    "    sort_ind_B = np.argsort(result_values_B)[::-1]\n",
    "    result_words_B = result_keys_B[sort_ind_B]\n",
    "    result_probs_B = result_values_B[sort_ind_B]\n",
    "    \n",
    "    return np.concatenate((result_words_A, result_words_B))[:n_words], np.concatenate((result_probs_A, result_probs_B))[:n_words]\n",
    "\n",
    "infill_results = np.asarray([filter_words(infill_results_raw[i], ex_sents_pos[i]) for i in range(infill_results_raw.shape[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each predicted word, look up the Oxford Dictionary to find an associated definition. If the word cannot be found, try its lemmatized and stemmed version. If all fails, the word itself is taken as the definition sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infill_vocab = set()\n",
    "\n",
    "for entry in infill_results:\n",
    "    for w in entry[0]:\n",
    "        infill_vocab.add(w)\n",
    "        \n",
    "infill_vocab = np.asarray(sorted(list(infill_vocab)))\n",
    "infill_vocab_inds = {infill_vocab[i]:i for i in range(infill_vocab.shape[0])}\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "cand_sentences = []\n",
    "cand_sent_map = {}\n",
    "c = 0\n",
    "\n",
    "for v in infill_vocab:\n",
    "    if v in oed_data.vocab:\n",
    "        word = v\n",
    "    elif lemmatizer.lemmatize(v) in oed_data.vocab:\n",
    "        word = lemmatizer.lemmatize(v)\n",
    "    elif stemmer.stem(lemmatizer.lemmatize(v)) in oed_data.vocab:\n",
    "        word = stemmer.stem(lemmatizer.lemmatize(v))\n",
    "    else:\n",
    "        word = None\n",
    "        c += 1\n",
    "    if word is None:\n",
    "        cand_sentences.append(v)\n",
    "    else:\n",
    "        cand_sentences.append(oed_data.data[word].definitions[0]['def'])\n",
    "    cand_sent_map[v] = cand_sentences[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training contrastive sense encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapt code from Sun et al. (2021) to train contrastive sense encodings (CSE) using training entries from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_encoder = FTEncoder('fastText/crawl-300d-2M-subword') # Points to the directory that contains downloaded fastText embeddings.\n",
    "trainer = SlangGenTrainer(dataset, word_encoder=ft_encoder, out_dir=out_dir, verbose=True)\n",
    "model = SlangGenModel(trainer, data_dir=out_dir)\n",
    "params = {'embed_name':'SBERT_contrastive', 'out_name':'predictions', 'model':'cf_prototype_5', 'prior':None, 'prior_name':'uniform', 'contr_params':None}\n",
    "model.train_contrastive(slang_inds, fold_name='udwil', params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: Train the original slang generation objective from Sun et al. (2021) to obtain good estimates for the kernel width parameters $h_m$ and $h_{cf}$. Note that this can be very memory intensive on the Urban Dictionary data because of its size. Can change params.model to 'prototype' instead to only estimate the $h_m$ parameter which is less memory intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_categorization(slang_inds, fold_name='udwil', params=params)\n",
    "\n",
    "p_dir = model.data_dir + '/' + fold_name + '/' + params['out_name'] + '/'\n",
    "\n",
    "with open(p_dir+\"parameters_\"+params['prior_name']+\".pkl\",\"rb\") as param_file:\n",
    "    gen_params = pickle.load(param_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Semantically informed reranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the appropriateness of each candidate interpretation (i.e. a predicted slang meaning) predicted by the language infill model against the slang's conventional meaning using the trained contrastive sense encoding with a prototype model. Apply collaborative filtering to take parallel semantic change into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_name='udwil'\n",
    "\n",
    "N_neighbors = 5\n",
    "neighbors = np.argsort(trainer.word_dist, axis=1)\n",
    "\n",
    "infill_cand_defs = []\n",
    "infill_word_ind = np.zeros((len(gt_words_test), N_neighbors), dtype=np.int32)\n",
    "\n",
    "for i in range(len(gt_words_test)):\n",
    "    infill_cand_defs.append([cand_sent_map[w] for w in infill_results[i][0]])\n",
    "    infill_word_ind[i] = neighbors[dataset.word2id[gt_words_test[i]], :N_neighbors]\n",
    "        \n",
    "infill_cand_defs = np.asarray(infill_cand_defs)\n",
    "\n",
    "h_model = gen_params['prototype'][0]\n",
    "h_word = 0.1\n",
    "\n",
    "# If you have enough RAM to estimate both parameters, here's what you would use:\n",
    "#h_model = gen_params['cf_prototype_5'][0]\n",
    "#h_word = gen_params['cf_prototype_5'][1]\n",
    "\n",
    "vd_vocab = normalize(np.exp(-1*trainer.word_dist/h_word), axis=1)\n",
    "\n",
    "vocab_embeds = model.load_exemplar_embeddings(fold_name=fold_name, params=params)\n",
    "vocab_proto = np.zeros((dataset.V, len(vocab_embeds[0][0])))\n",
    "for i in range(dataset.V):\n",
    "    vocab_proto[i] = np.mean(vocab_embeds[i], axis=0)\n",
    "\n",
    "preds = np.zeros((len(gt_words_test), infill_results.shape[2]))\n",
    "\n",
    "for k in trange(len(gt_words_test)):\n",
    "\n",
    "    prototypes = trainer.get_testtime_embeddings(infill_cand_defs[k], fold_name=fold_name)\n",
    "    queries = vocab_proto[infill_word_ind[k]]\n",
    "\n",
    "    N_query = queries.shape[0]\n",
    "    vd_prototype = np.zeros((N_query, prototypes.shape[0]))\n",
    "\n",
    "    for i in range(N_query):\n",
    "        vd_prototype[i] = np.linalg.norm(prototypes - queries[i], axis=1)\n",
    "    vd_prototype = -1*vd_prototype**2\n",
    "\n",
    "    l_prototype = normalize(np.exp(vd_prototype/h_model), axis=1)\n",
    "\n",
    "    cf_weights = vd_vocab[infill_word_ind[k, 0], infill_word_ind[k]]\n",
    "    preds[k] = normalize_1d(np.sum(l_prototype * normalize(cf_weights[:, np.newaxis], axis=0), axis=0))\n",
    "    \n",
    "np.save(o_dir+'interp_lm_ssi.npy', preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain Sentence-BERT embeddings of all definitions sentences involved in the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "train_dev_sents = np.asarray([dataset.slang_data[i].def_sent for i in train_dev_ind])\n",
    "train_dev_embeds = normalize_L2(np.asarray(sbert_model.encode(train_dev_sents)))\n",
    "\n",
    "test_sents = np.asarray([dataset.slang_data[i].def_sent for i in slang_inds.test])\n",
    "test_embeds = normalize_L2((sbert_model.encode(test_sents)))\n",
    "\n",
    "vocab_base_embeds = normalize_L2(np.asarray(sbert_model.encode(cand_sentences)))\n",
    "\n",
    "infill_base_embed = np.zeros((infill_results.shape[0], 50, vocab_base_embeds.shape[1]))\n",
    "\n",
    "for i in range(infill_results.shape[0]):\n",
    "    for j in range(50):\n",
    "        infill_base_embed[i, j] = vocab_base_embeds[infill_vocab_inds[infill_results[i][0][j]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each test entry, sample 4 negative definition sentences from the training set and the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_close_def(query_sent, target_sent, threshold=0.5):\n",
    "    query_s = [w for w in simple_preprocess(query_sent) if w not in stopwords]\n",
    "    target_s = set([w for w in simple_preprocess(target_sent) if w not in stopwords])\n",
    "    overlap_c = 0\n",
    "    for word in query_s:\n",
    "        if word in target_s:\n",
    "            overlap_c += 1\n",
    "    return overlap_c >= len(query_s) * threshold\n",
    "\n",
    "def sample_def(ref, num_samples=4):\n",
    "    N_cand = train_dev_embeds.shape[0]\n",
    "    samples = []\n",
    "    ref_sent = dataset.slang_data[slang_inds.test[ref]].def_sent\n",
    "    while len(samples) < num_samples:\n",
    "        new_sample = np.random.randint(N_cand)\n",
    "        new_sent = train_dev_sents[new_sample]\n",
    "        # Comment out the following two lines for a completely random sample\n",
    "        if not is_close_def(ref_sent, new_sent):\n",
    "            samples.append(new_sample)\n",
    "        samples.append(new_sample)\n",
    "    return samples\n",
    "\n",
    "test_neg_samples = np.asarray([sample_def(i) for i in range(slang_inds.test.shape[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function used to compute and rank the semantic distance between the predicted definition again the groundtruth definition and 4 other negatively sampled definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pred_ranks(pred, n=50):\n",
    "    pred_ranks = np.empty((len(ex_sents_inds), n), dtype=np.int32)\n",
    "\n",
    "    for i in range(len(ex_sents_inds)):\n",
    "        pred_embeds = infill_base_embed[i][pred[i, :n]]\n",
    "        cand_embeds = np.concatenate((test_embeds[ex_sents_inds[i]][np.newaxis, :], train_dev_embeds[test_neg_samples[ex_sents_inds[i]]]))\n",
    "        _, ranks = np.where(np.argsort(dist.cdist(pred_embeds, cand_embeds), axis=1)==0)\n",
    "        ranks = ranks + 1\n",
    "        pred_ranks[i] = ranks\n",
    "\n",
    "    return pred_ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute and print the mean reciprocal rank (MRR) results for both the baseline and the semantically informed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_interp_results(sg_probs, alpha = 0.5, epsilon = 1e-7):\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    prob_baseline = np.asarray(infill_results[:,1,:], dtype=np.float32) + epsilon\n",
    "    prob_ssi = sg_probs + epsilon\n",
    "\n",
    "    pred_baseline = np.argsort(normalize(prob_baseline), axis=1)[:, ::-1]\n",
    "    pred_ssi = np.argsort(normalize(prob_ssi), axis=1)[:, ::-1]\n",
    "\n",
    "    pred_baseline_ranks = compute_pred_ranks(pred_baseline)\n",
    "    pred_ssi_ranks = compute_pred_ranks(pred_ssi)\n",
    "    \n",
    "    results['pred_baseline'] = pred_baseline\n",
    "    results['pred_ssi'] = pred_ssi\n",
    "    \n",
    "    results['pred_baseline_ranks'] = pred_baseline_ranks\n",
    "    results['pred_ssi_ranks'] = pred_ssi_ranks\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_results_interp_mrr(ranks, model_name='default'):\n",
    "    print(model_name+\" - top 1: %f\" % np.mean(1/ranks[:,0]))\n",
    "    \n",
    "def eval_results_interp_mrr(results):\n",
    "    print(\"---\")\n",
    "    print_results_interp_mrr(results['pred_baseline_ranks'], 'LM Infill')\n",
    "    print(\"---\")\n",
    "    print_results_interp_mrr(results['pred_ssi_ranks'], 'LM Infill + SSI')\n",
    "    print(\"---\")\n",
    "    \n",
    "sg_probs = np.load(o_dir+'interp_lm_ssi.npy')\n",
    "results = compute_interp_results(sg_probs)\n",
    "eval_results_interp_mrr(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chance MRR is 0.457 as computed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MRR - chance\n",
    "np.mean(1/np.arange(1,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slang interpretation results for the i'th test example can be retrieved as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "# Plain word predictions\n",
    "print('[LM Infill] '+','.join(infill_results[i, 0][results['pred_baseline'][i,:5]]))\n",
    "print('[LM Infill + SSI] '+','.join(infill_results[i, 0][results['pred_ssi'][i,:5]]))\n",
    "\n",
    "# Predictions after dictionary lookup\n",
    "print('[LM Infill]\\n'+'\\n'.join([cand_sent_map[w] for w in infill_results[i, 0][results['pred_baseline'][i,:5]]]))\n",
    "print('[LM Infill + SSI]\\n'+'\\n'.join([cand_sent_map[w] for w in infill_results[i, 0][results['pred_ssi'][i,:5]]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
