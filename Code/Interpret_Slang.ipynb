{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary code package for paper submission: 'Semantically Informed Slang Interpretation'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the supplementary code package for 'Semantically Informed Slang Interpretation'. Since we cannot publically release all revelant datasets used in the study due to copyright terms, the purpose of this notebook is to provide an illustration of how the main results from the paper can be reproduced. Specifically, the code package includes all required non-standard code dependencies and code in this notebook show how results can be reproduced using these libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We include custom versions of code released from previous work in the following directories:\n",
    "\n",
    "- **/CatGO** - A categorization library from Zhewei Sun, Richard Zemel, and Yang Xu, 'Slang generation as categorization', 2019.\n",
    "- **/ilm** - A pre-trained GPT-2 based language infill model from Chris Donahue, Mina Lee, and Percy Liang, 'Enabling language models to fill in the blanks', 2020\n",
    "- **/slanggen** - A library for training contrastively learned slang sense embeddings from Zhewei Sun, Richard Zemel, and Yang Xu, 'A computational framework for slang generation', 2021."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a list of scientific Python packages you'll need:\n",
    "\n",
    "- numpy\n",
    "- scipy\n",
    "- nltk\n",
    "- gensim\n",
    "- Flair\n",
    "- PyTorch\n",
    "- transformers\n",
    "- sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "import numpy as np\n",
    "import scipy.spatial.distance as dist\n",
    "import scipy.stats\n",
    "from scipy.stats import norm, mode\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from collections import defaultdict, namedtuple, Counter, defaultdict\n",
    "\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "from CatGO.categorize import Categorizer\n",
    "\n",
    "from slanggen.util import *\n",
    "from slanggen.dataloader import UD_Wil_Dataset, OED_Dataset\n",
    "from slanggen.encoder import FTEncoder\n",
    "from slanggen.contrastive import SlangGenTrainer\n",
    "from slanggen.model import SlangGenModel\n",
    "\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "import ilm.tokenize_util\n",
    "import ilm.infer\n",
    "\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads the dataset and assosiated indices for data splits. Note that the Oxford Dictionary (OD) data cannot be included so this is just an illustration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDefinition:\n",
    "    def __init__(self, word, definitions):\n",
    "        self.word = word\n",
    "        self.definitions = definitions\n",
    "\n",
    "# Create a list of WordDefinition objects.\n",
    "data = [\n",
    "    WordDefinition(\"LOL\", \"Laugh out loud\"),\n",
    "    WordDefinition(\"BRB\", \"Be right back\"),\n",
    "    WordDefinition(\"IDK\", \"I don't know\"),\n",
    "    # Add more entries as needed...\n",
    "]\n",
    "\n",
    "# Convert the list to a NumPy array.\n",
    "data_array = np.array(data, dtype=object)\n",
    "\n",
    "# Save the array to a .npy file.\n",
    "np.save(\"OED_Urban_def_full.npy\", data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<__main__.WordDefinition object at 0x7f8bd0487850>\n",
      " <__main__.WordDefinition object at 0x7f8bd04879a0>\n",
      " <__main__.WordDefinition object at 0x7f8bd04873a0>]\n",
      "{'BRB', 'LOL', 'IDK'}\n",
      "(61025, 3) (1242, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ronpechuk/src/447-Final-Project/Code/slanggen/dataloader.py:143: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data_train = pd.read_csv(slang_path+'UD_train.csv', error_bad_lines=False, usecols=[0,1,2]).values\n",
      "/Users/ronpechuk/src/447-Final-Project/Code/slanggen/dataloader.py:144: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data_test = pd.read_csv(slang_path+'UD_test.csv', error_bad_lines=False, usecols=[0,1,2]).values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  [SlangEntry(word='tilt', def_sent=\"term used for pissed off when gambling, could be used when someone isn't thinking straight \", meta_data={'context': ['my girlfriend started blowing me and then she stopped, i went on [*SLANGAAAP*] and donkey punched her.']}), SlangEntry(word='bloom', def_sent=\"(verb) the act of gaping a woman's anus so much that when you pull out, her innards literally 'bloom' out of her like a flower opening it's pedals. also referred to as an [ass tulip].\", meta_data={'context': [\"that chick didn't just get gaped, i made that ass [*SLANGAAAP*]!\"]}), SlangEntry(word='cockchafer', def_sent='an unsightly guy at a swimming pool with far too tight speedos, in clearly noticeable pain, but thinking the chicks are digging him.', meta_data={'context': [\"maybe things could have worked out, but i'm a keen swimmer at the local pool and he was a [*SLANGAAAP*]\"]}), SlangEntry(word='cob', def_sent='circle of bosses. crip over blood. conductin organize business. cash over bitches. controlin our block. cartel of ballaz.  wordplay and set of definitions for \"c.o.b.\" used by rapper crooked i.', meta_data={'context': ['crooked i\\'s hip hop weekly freestyle series. \"kill us all\"   ay yo [*SLANGAAAP*] ']}), SlangEntry(word='homeless', def_sent='a person who hangs out at a psychosocial clubhouse all day to sleep on the couch or surf the internet all day. the homeless people at a clubhouse are mostly veterans!  ', meta_data={'context': [\"why should i go to the clubhouse? i'm not [*SLANGAAAP*]!\"]}), SlangEntry(word='breakout', def_sent='ready to make many contributions, ready to step in and take the spotlight.', meta_data={'context': ['santana moss is ready for a [*SLANGAAAP*] year this year.']}), SlangEntry(word='ballpark', def_sent='a sex act performed during ejaculation.  the guy is doing the girl \"doggy-style\" and when he starts to cum, he pulls out, rests his dick between the two cheeks of the girl\\'s ass and cums onto her back or asshole. the term \"ballpark\" is used because the dick between the buttcheeks resembles a ballpark hot dog in a bun.. one game guys can play with this is to see how far he can cum on her back.. make a note, compare with friends.', meta_data={'context': ['i was fucking this girl last night and i totally gave her the [*SLANGAAAP*].']}), SlangEntry(word='balkanize', def_sent=\"to split something (usually a state) apart, on a usually ethnic basis. this verb first appeared in the 1990's during the war in bosnia, led by croats, serbs and bosnians, which caused the demise of the socialist federal republic of yugoslavia (sfry). it is nowadays again popular with kosovar albanians trying to rip off a part of serbia -kosovo (15% of serbian territory, imagine the us without texas)- for themselves.\", meta_data={'context': ['it seems highly unlikely to [*SLANGAAAP*] the usa.']}), SlangEntry(word='trim', def_sent='get laid, sexual intercourse, pussy', meta_data={'context': ['eddie murphy in \"48 hours\":  \"i\\'ve got to get me some [*SLANGAAAP*], man!\"']}), SlangEntry(word='guard', def_sent='a former adventurer who took an arrow in the knee.', meta_data={'context': ['[*SLANGAAAP*]: \"i used to be an adventurer like you, then i took an arrow in the knee.\"']})]\n"
     ]
    }
   ],
   "source": [
    "oed_data = OED_Dataset('OED_Urban_def_full.npy') # TODO fix the fact that we have these \"WordDefinition\" objects also I think we need to just use the UD data here with dev and train combined\n",
    "\n",
    "ud_dir = '../'\n",
    "dataset = UD_Wil_Dataset(ud_dir+'Data/', oed_data, load_oov=True)\n",
    "\n",
    "# ========== create the splits ==========\n",
    "print(\"data: \", dataset.slang_data_oov[0:10]) # TODO fix the fact that dataset.slange_data is empty\n",
    "data_length = len(dataset.slang_data)\n",
    "\n",
    "# Determine the sizes of each split\n",
    "train_size = int(data_length * 0.7)\n",
    "dev_size = int(data_length * 0.15)\n",
    "\n",
    "# Create index arrays\n",
    "indices = np.arange(data_length)\n",
    "np.random.shuffle(indices)\n",
    "train_ind = indices[:train_size]\n",
    "dev_ind = indices[train_size:train_size+dev_size]\n",
    "test_ind = indices[train_size+dev_size:]\n",
    "\n",
    "# Save index arrays as .npy files\n",
    "np.save(ud_dir+'train_ind.npy', train_ind)\n",
    "np.save(ud_dir+'dev_ind.npy', dev_ind)\n",
    "np.save(ud_dir+'test_ind.npy', test_ind)\n",
    "# ========== create the splits ==========\n",
    "\n",
    "slang_inds = DataIndex(np.load(ud_dir+'train_ind.npy'), np.load(ud_dir+'dev_ind.npy'), np.load(ud_dir+'test_ind.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra data bookkeeping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dev_ind = np.concatenate((slang_inds.train, slang_inds.dev))\n",
    "\n",
    "ex_sents_test = []\n",
    "ex_sents_inds = []\n",
    "gt_words_test = []\n",
    "\n",
    "for i in range(slang_inds.test.shape[0]):\n",
    "    ind = slang_inds.test[i]\n",
    "    for s in dataset.slang_data[ind].meta_data['context']:\n",
    "        ex_sents_test.append(s.strip())\n",
    "        ex_sents_inds.append(i)\n",
    "        gt_words_test.append(dataset.slang_data[ind].word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain interpretation candidates from a pre-trained language infill model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a POS tagger and tag the blanked out slang expression in the context sentence for every test entry. Note that the slang expression itself is not provided to the tagger to mitigate potential biases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penn_pos_map = {'JJ':'adj',\\\n",
    "                'JJR':'adj',\\\n",
    "                'JJS':'adj',\\\n",
    "                'UH':'interj',\\\n",
    "                'RB':'adv',\\\n",
    "                'RBR':'adv',\\\n",
    "                'RBS':'adv',\\\n",
    "                'WRB':'adv',\\\n",
    "                'NN':'noun',\\\n",
    "                'NNS':'noun',\\\n",
    "                'NNP':'noun',\\\n",
    "                'NNPS':'noun',\\\n",
    "                'MD':'verb',\\\n",
    "                'VB':'verb',\\\n",
    "                'VBD':'verb',\\\n",
    "                'VBG':'verb',\\\n",
    "                'VBN':'verb',\\\n",
    "                'VBP':'verb',\\\n",
    "                'VBZ':'verb'}\n",
    "\n",
    "def conv_penn_pos(tag):\n",
    "    if tag in penn_pos_map:\n",
    "        return penn_pos_map[tag]\n",
    "    return 'other'\n",
    "\n",
    "tagger = SequenceTagger.load('pos')\n",
    "\n",
    "punctuations = '!\"#$%&()\\*\\+,-\\./:;<=>?@[\\\\]^_`{|}~'\n",
    "re_punc = re.compile(r\"[\"+punctuations+r\"]+\")\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return re.compile(r\"(?:^|(?<=\\s))\\S+(?=\\s|$)\").findall(sentence)\n",
    "\n",
    "ex_sents_pos = []\n",
    "\n",
    "for s in ex_sents_test:\n",
    "    \n",
    "    sent = re_punc.sub('', s)\n",
    "    gap_pos = 0\n",
    "    for j, token in enumerate(tokenize(sent)):\n",
    "        if len(token) >= 9:\n",
    "            if token[:9] == 'SLANGAAAP':\n",
    "                gap_pos = j\n",
    "                break\n",
    "    sent = Sentence(re.compile('SLANGAAAP').sub('slanggg', sent))\n",
    "    tagger.predict(sent)\n",
    "    tag_pred = conv_penn_pos(sent.get_spans('pos')[gap_pos].tag)\n",
    "    \n",
    "    ex_sents_pos.append(tag_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a pre-trained language infill model from Donahue et al. (2020). The model can be downloaded from their original repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = 'pretrain/models/sto_ilm' # Change this to where you have placed the pre-trained model\n",
    "MASK_CLS = 'ilm.mask.hierarchical.MaskHierarchical'\n",
    "\n",
    "tokenizer = ilm.tokenize_util.Tokenizer.GPT2\n",
    "with open(os.path.join(MODEL_DIR, 'additional_ids_to_tokens.pkl'), 'rb') as f:\n",
    "    additional_ids_to_tokens = pickle.load(f)\n",
    "additional_tokens_to_ids = {v:k for k, v in additional_ids_to_tokens.items()}\n",
    "try:\n",
    "    ilm.tokenize_util.update_tokenizer(additional_ids_to_tokens, tokenizer)\n",
    "except ValueError:\n",
    "    print('Already updated')\n",
    "print(additional_tokens_to_ids)\n",
    "\n",
    "_blank_id = ilm.tokenize_util.encode(' _', tokenizer)[0]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_DIR)\n",
    "model.eval()\n",
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the language infill model to obtain a list of infilled words for each test example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infill_prob(sentence, n_words=5):\n",
    "\n",
    "    # Preprocess Sentence\n",
    "\n",
    "    context = sentence.replace('[*SLANGAAAP*]', ' _ ')\n",
    "\n",
    "    context_ids = ilm.tokenize_util.encode(context, tokenizer)\n",
    "\n",
    "    context_ids[context_ids.index(_blank_id)] = additional_tokens_to_ids['<|infill_word|>']\n",
    "\n",
    "    # Obtain Probability Distribution from Softmax\n",
    "\n",
    "    probs = ilm.infer.infill_with_ilm(\n",
    "        model,\n",
    "        additional_tokens_to_ids,\n",
    "        context_ids,\n",
    "        num_infills=1).cpu().numpy()[0]\n",
    "\n",
    "    # Collect Words and Probabilities\n",
    "    \n",
    "    top_probs = np.argsort(probs)[::-1]\n",
    "    top_words = ilm.tokenize_util.ids_to_tokens(top_probs[:n_words], tokenizer)\n",
    "  \n",
    "    return probs[top_probs[:n_words]], top_words\n",
    "\n",
    "infill_results_raw = np.asarray([infill_prob(sent, 150) for sent in ex_sents_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First filter out words that contain non-alphanumeric characters. Then, check the part-of-speech (POS) tag predicted from the usage context to see if it matches the candadiate words. Words with matching POS tags are moved to the front of the list. Finally, keep the top 50 candidate words for each test example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'histwords/eng-all/pos/1990-pos_counts.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mother\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# You'll need to download this from the HistWord Project by Hamilton et al. (2016).\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m hist_counts \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhistwords/eng-all/pos/1990-pos_counts.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     23\u001b[0m hist_vocab \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(hist_counts\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_hist_posdist\u001b[39m(word):\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'histwords/eng-all/pos/1990-pos_counts.pkl'"
     ]
    }
   ],
   "source": [
    "def alphanum_check(w):\n",
    "    if len(w) == 0:\n",
    "        return False\n",
    "    for c in w:\n",
    "        c_num = ord(c)\n",
    "        if not ((c_num >= 48 and c_num <= 57) or (c_num >= 65 and c_num <= 90) or (c_num >= 97 and c_num <= 122)):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "hist_pos_map = {'ADJ':'adj',\\\n",
    "                'X':'interj',\\\n",
    "                'ADV':'adv',\\\n",
    "                'VERB':'verb',\\\n",
    "                'NOUN':'noun'}\n",
    "\n",
    "def conv_hist_pos(tag):\n",
    "    if tag in hist_pos_map:\n",
    "        return hist_pos_map[tag]\n",
    "    return 'other'\n",
    "\n",
    "# You'll need to download this from the HistWord Project by Hamilton et al. (2016).\n",
    "hist_counts = pickle.load(open('histwords/eng-all/pos/1990-pos_counts.pkl', 'rb'))\n",
    "hist_vocab = set(hist_counts.keys())\n",
    "\n",
    "def get_hist_posdist(word):\n",
    "    results = defaultdict(float)\n",
    "    for key,val in hist_counts[word].items():\n",
    "        results[conv_hist_pos(key)] += val\n",
    "    total = np.sum(list(results.values()))\n",
    "    for key in results.keys():\n",
    "        results[key] /= total\n",
    "    return results\n",
    "\n",
    "def pos_check(word, tag, threshold=0.05):\n",
    "    if word not in hist_vocab:\n",
    "        return False\n",
    "    hist_posdist = get_hist_posdist(word)\n",
    "    return hist_posdist[tag] >= threshold\n",
    "\n",
    "def filter_words(result, tag, n_words=50):\n",
    "    probs = np.asarray(result[0])\n",
    "    words = np.asarray([s.strip() for s in result[1]])\n",
    "    result_mask = np.arange(len(result[1]), dtype=np.int32)\n",
    "    good_pos = set()\n",
    "\n",
    "    for i in range(result_mask.shape[0]):\n",
    "        if not alphanum_check(words[i]):\n",
    "            result_mask[i] = -1\n",
    "        if pos_check(words[i], tag):\n",
    "            good_pos.add(i)\n",
    "\n",
    "    result_mask = np.asarray([i for i in result_mask if i != -1], dtype=np.int32)\n",
    "    \n",
    "    mask_A = np.asarray([i for i in result_mask if i in good_pos], dtype=np.int32)\n",
    "    mask_B = np.asarray([i for i in result_mask if i not in good_pos], dtype=np.int32)\n",
    "    \n",
    "    probs_A = probs[mask_A]\n",
    "    words_A = words[mask_A]\n",
    "    \n",
    "    probs_B = probs[mask_B]\n",
    "    words_B = words[mask_B]\n",
    "\n",
    "    result_dict_A = defaultdict(float)\n",
    "    for i in range(words_A.shape[0]):\n",
    "        result_dict_A[words_A[i].lower()] += float(probs_A[i])\n",
    "    result_dict_B = defaultdict(float)\n",
    "    for i in range(words_B.shape[0]):\n",
    "        result_dict_B[words_B[i].lower()] += float(probs_B[i])\n",
    "\n",
    "    result_keys_A = np.asarray(list(result_dict_A.keys()))\n",
    "    result_values_A = np.asarray(list(result_dict_A.values()))\n",
    "    result_keys_B = np.asarray(list(result_dict_B.keys()))\n",
    "    result_values_B = np.asarray(list(result_dict_B.values()))\n",
    "\n",
    "    sort_ind_A = np.argsort(result_values_A)[::-1]\n",
    "    result_words_A = result_keys_A[sort_ind_A]\n",
    "    result_probs_A = result_values_A[sort_ind_A]\n",
    "    \n",
    "    sort_ind_B = np.argsort(result_values_B)[::-1]\n",
    "    result_words_B = result_keys_B[sort_ind_B]\n",
    "    result_probs_B = result_values_B[sort_ind_B]\n",
    "    \n",
    "    return np.concatenate((result_words_A, result_words_B))[:n_words], np.concatenate((result_probs_A, result_probs_B))[:n_words]\n",
    "\n",
    "infill_results = np.asarray([filter_words(infill_results_raw[i], ex_sents_pos[i]) for i in range(infill_results_raw.shape[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each predicted word, look up the Oxford Dictionary to find an associated definition. If the word cannot be found, try its lemmatized and stemmed version. If all fails, the word itself is taken as the definition sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infill_vocab = set()\n",
    "\n",
    "for entry in infill_results:\n",
    "    for w in entry[0]:\n",
    "        infill_vocab.add(w)\n",
    "        \n",
    "infill_vocab = np.asarray(sorted(list(infill_vocab)))\n",
    "infill_vocab_inds = {infill_vocab[i]:i for i in range(infill_vocab.shape[0])}\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "cand_sentences = []\n",
    "cand_sent_map = {}\n",
    "c = 0\n",
    "\n",
    "for v in infill_vocab:\n",
    "    if v in oed_data.vocab:\n",
    "        word = v\n",
    "    elif lemmatizer.lemmatize(v) in oed_data.vocab:\n",
    "        word = lemmatizer.lemmatize(v)\n",
    "    elif stemmer.stem(lemmatizer.lemmatize(v)) in oed_data.vocab:\n",
    "        word = stemmer.stem(lemmatizer.lemmatize(v))\n",
    "    else:\n",
    "        word = None\n",
    "        c += 1\n",
    "    if word is None:\n",
    "        cand_sentences.append(v)\n",
    "    else:\n",
    "        cand_sentences.append(oed_data.data[word].definitions[0]['def'])\n",
    "    cand_sent_map[v] = cand_sentences[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training contrastive sense encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapt code from Sun et al. (2021) to train contrastive sense encodings (CSE) using training entries from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_encoder = FTEncoder('fastText/crawl-300d-2M-subword') # Points to the directory that contains downloaded fastText embeddings.\n",
    "trainer = SlangGenTrainer(dataset, word_encoder=ft_encoder, out_dir=out_dir, verbose=True)\n",
    "model = SlangGenModel(trainer, data_dir=out_dir)\n",
    "params = {'embed_name':'SBERT_contrastive', 'out_name':'predictions', 'model':'cf_prototype_5', 'prior':None, 'prior_name':'uniform', 'contr_params':None}\n",
    "model.train_contrastive(slang_inds, fold_name='udwil', params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: Train the original slang generation objective from Sun et al. (2021) to obtain good estimates for the kernel width parameters $h_m$ and $h_{cf}$. Note that this can be very memory intensive on the Urban Dictionary data because of its size. Can change params.model to 'prototype' instead to only estimate the $h_m$ parameter which is less memory intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train_categorization(slang_inds, fold_name='udwil', params=params)\n",
    "\n",
    "p_dir = model.data_dir + '/' + fold_name + '/' + params['out_name'] + '/'\n",
    "\n",
    "with open(p_dir+\"parameters_\"+params['prior_name']+\".pkl\",\"rb\") as param_file:\n",
    "    gen_params = pickle.load(param_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Semantically informed reranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the appropriateness of each candidate interpretation (i.e. a predicted slang meaning) predicted by the language infill model against the slang's conventional meaning using the trained contrastive sense encoding with a prototype model. Apply collaborative filtering to take parallel semantic change into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_name='udwil'\n",
    "\n",
    "N_neighbors = 5\n",
    "neighbors = np.argsort(trainer.word_dist, axis=1)\n",
    "\n",
    "infill_cand_defs = []\n",
    "infill_word_ind = np.zeros((len(gt_words_test), N_neighbors), dtype=np.int32)\n",
    "\n",
    "for i in range(len(gt_words_test)):\n",
    "    infill_cand_defs.append([cand_sent_map[w] for w in infill_results[i][0]])\n",
    "    infill_word_ind[i] = neighbors[dataset.word2id[gt_words_test[i]], :N_neighbors]\n",
    "        \n",
    "infill_cand_defs = np.asarray(infill_cand_defs)\n",
    "\n",
    "h_model = gen_params['prototype'][0]\n",
    "h_word = 0.1\n",
    "\n",
    "# If you have enough RAM to estimate both parameters, here's what you would use:\n",
    "#h_model = gen_params['cf_prototype_5'][0]\n",
    "#h_word = gen_params['cf_prototype_5'][1]\n",
    "\n",
    "vd_vocab = normalize(np.exp(-1*trainer.word_dist/h_word), axis=1)\n",
    "\n",
    "vocab_embeds = model.load_exemplar_embeddings(fold_name=fold_name, params=params)\n",
    "vocab_proto = np.zeros((dataset.V, len(vocab_embeds[0][0])))\n",
    "for i in range(dataset.V):\n",
    "    vocab_proto[i] = np.mean(vocab_embeds[i], axis=0)\n",
    "\n",
    "preds = np.zeros((len(gt_words_test), infill_results.shape[2]))\n",
    "\n",
    "for k in trange(len(gt_words_test)):\n",
    "\n",
    "    prototypes = trainer.get_testtime_embeddings(infill_cand_defs[k], fold_name=fold_name)\n",
    "    queries = vocab_proto[infill_word_ind[k]]\n",
    "\n",
    "    N_query = queries.shape[0]\n",
    "    vd_prototype = np.zeros((N_query, prototypes.shape[0]))\n",
    "\n",
    "    for i in range(N_query):\n",
    "        vd_prototype[i] = np.linalg.norm(prototypes - queries[i], axis=1)\n",
    "    vd_prototype = -1*vd_prototype**2\n",
    "\n",
    "    l_prototype = normalize(np.exp(vd_prototype/h_model), axis=1)\n",
    "\n",
    "    cf_weights = vd_vocab[infill_word_ind[k, 0], infill_word_ind[k]]\n",
    "    preds[k] = normalize_1d(np.sum(l_prototype * normalize(cf_weights[:, np.newaxis], axis=0), axis=0))\n",
    "    \n",
    "np.save(o_dir+'interp_lm_ssi.npy', preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain Sentence-BERT embeddings of all definitions sentences involved in the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "train_dev_sents = np.asarray([dataset.slang_data[i].def_sent for i in train_dev_ind])\n",
    "train_dev_embeds = normalize_L2(np.asarray(sbert_model.encode(train_dev_sents)))\n",
    "\n",
    "test_sents = np.asarray([dataset.slang_data[i].def_sent for i in slang_inds.test])\n",
    "test_embeds = normalize_L2((sbert_model.encode(test_sents)))\n",
    "\n",
    "vocab_base_embeds = normalize_L2(np.asarray(sbert_model.encode(cand_sentences)))\n",
    "\n",
    "infill_base_embed = np.zeros((infill_results.shape[0], 50, vocab_base_embeds.shape[1]))\n",
    "\n",
    "for i in range(infill_results.shape[0]):\n",
    "    for j in range(50):\n",
    "        infill_base_embed[i, j] = vocab_base_embeds[infill_vocab_inds[infill_results[i][0][j]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each test entry, sample 4 negative definition sentences from the training set and the dev set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_close_def(query_sent, target_sent, threshold=0.5):\n",
    "    query_s = [w for w in simple_preprocess(query_sent) if w not in stopwords]\n",
    "    target_s = set([w for w in simple_preprocess(target_sent) if w not in stopwords])\n",
    "    overlap_c = 0\n",
    "    for word in query_s:\n",
    "        if word in target_s:\n",
    "            overlap_c += 1\n",
    "    return overlap_c >= len(query_s) * threshold\n",
    "\n",
    "def sample_def(ref, num_samples=4):\n",
    "    N_cand = train_dev_embeds.shape[0]\n",
    "    samples = []\n",
    "    ref_sent = dataset.slang_data[slang_inds.test[ref]].def_sent\n",
    "    while len(samples) < num_samples:\n",
    "        new_sample = np.random.randint(N_cand)\n",
    "        new_sent = train_dev_sents[new_sample]\n",
    "        # Comment out the following two lines for a completely random sample\n",
    "        if not is_close_def(ref_sent, new_sent):\n",
    "            samples.append(new_sample)\n",
    "        samples.append(new_sample)\n",
    "    return samples\n",
    "\n",
    "test_neg_samples = np.asarray([sample_def(i) for i in range(slang_inds.test.shape[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function used to compute and rank the semantic distance between the predicted definition again the groundtruth definition and 4 other negatively sampled definitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pred_ranks(pred, n=50):\n",
    "    pred_ranks = np.empty((len(ex_sents_inds), n), dtype=np.int32)\n",
    "\n",
    "    for i in range(len(ex_sents_inds)):\n",
    "        pred_embeds = infill_base_embed[i][pred[i, :n]]\n",
    "        cand_embeds = np.concatenate((test_embeds[ex_sents_inds[i]][np.newaxis, :], train_dev_embeds[test_neg_samples[ex_sents_inds[i]]]))\n",
    "        _, ranks = np.where(np.argsort(dist.cdist(pred_embeds, cand_embeds), axis=1)==0)\n",
    "        ranks = ranks + 1\n",
    "        pred_ranks[i] = ranks\n",
    "\n",
    "    return pred_ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute and print the mean reciprocal rank (MRR) results for both the baseline and the semantically informed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_interp_results(sg_probs, alpha = 0.5, epsilon = 1e-7):\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    prob_baseline = np.asarray(infill_results[:,1,:], dtype=np.float32) + epsilon\n",
    "    prob_ssi = sg_probs + epsilon\n",
    "\n",
    "    pred_baseline = np.argsort(normalize(prob_baseline), axis=1)[:, ::-1]\n",
    "    pred_ssi = np.argsort(normalize(prob_ssi), axis=1)[:, ::-1]\n",
    "\n",
    "    pred_baseline_ranks = compute_pred_ranks(pred_baseline)\n",
    "    pred_ssi_ranks = compute_pred_ranks(pred_ssi)\n",
    "    \n",
    "    results['pred_baseline'] = pred_baseline\n",
    "    results['pred_ssi'] = pred_ssi\n",
    "    \n",
    "    results['pred_baseline_ranks'] = pred_baseline_ranks\n",
    "    results['pred_ssi_ranks'] = pred_ssi_ranks\n",
    "    \n",
    "    return results\n",
    "\n",
    "def print_results_interp_mrr(ranks, model_name='default'):\n",
    "    print(model_name+\" - top 1: %f\" % np.mean(1/ranks[:,0]))\n",
    "    \n",
    "def eval_results_interp_mrr(results):\n",
    "    print(\"---\")\n",
    "    print_results_interp_mrr(results['pred_baseline_ranks'], 'LM Infill')\n",
    "    print(\"---\")\n",
    "    print_results_interp_mrr(results['pred_ssi_ranks'], 'LM Infill + SSI')\n",
    "    print(\"---\")\n",
    "    \n",
    "sg_probs = np.load(o_dir+'interp_lm_ssi.npy')\n",
    "results = compute_interp_results(sg_probs)\n",
    "eval_results_interp_mrr(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chance MRR is 0.457 as computed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MRR - chance\n",
    "np.mean(1/np.arange(1,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slang interpretation results for the i'th test example can be retrieved as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "# Plain word predictions\n",
    "print('[LM Infill] '+','.join(infill_results[i, 0][results['pred_baseline'][i,:5]]))\n",
    "print('[LM Infill + SSI] '+','.join(infill_results[i, 0][results['pred_ssi'][i,:5]]))\n",
    "\n",
    "# Predictions after dictionary lookup\n",
    "print('[LM Infill]\\n'+'\\n'.join([cand_sent_map[w] for w in infill_results[i, 0][results['pred_baseline'][i,:5]]]))\n",
    "print('[LM Infill + SSI]\\n'+'\\n'.join([cand_sent_map[w] for w in infill_results[i, 0][results['pred_ssi'][i,:5]]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
